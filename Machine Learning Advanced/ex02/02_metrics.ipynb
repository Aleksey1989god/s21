{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 09. Exercise 02\n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:27.141791Z",
     "start_time": "2025-11-11T17:47:26.299837Z"
    }
   },
   "source": [
    "import pandas as pd     \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier   \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import joblib    "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the same dataframe as in the previous exercise.\n",
    "2. Using `train_test_split` with parameters `test_size=0.2`, `random_state=21` get `X_train`, `y_train`, `X_test`, `y_test`. Use the additional parameter `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:27.173309Z",
     "start_time": "2025-11-11T17:47:27.149792Z"
    }
   },
   "source": "df = pd.read_csv('../data/day-of-week-not-scaled.csv')",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:27.373833Z",
     "start_time": "2025-11-11T17:47:27.359832Z"
    }
   },
   "source": [
    "df_day = pd.read_csv('../data/dayofweek.csv')\n",
    "df['dayofweek'] = df_day['dayofweek']"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:27.421655Z",
     "start_time": "2025-11-11T17:47:27.407670Z"
    }
   },
   "source": [
    "X = df.drop('dayofweek', axis=1)\n",
    "y = df['dayofweek']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:27.483655Z",
     "start_time": "2025-11-11T17:47:27.469658Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the best parameters from the previous exercise and train the model of SVM.\n",
    "2. You need to calculate `accuracy`, `precision`, `recall`, `ROC AUC`.\n",
    "\n",
    " - `precision` and `recall` should be calculated for each class (use `average='weighted'`)\n",
    " - `ROC AUC` should be calculated for each class against any other class (all possible pairwise combinations) and then weighted average should be applied for the final metric\n",
    " - the code in the cell should display the result as below:\n",
    "\n",
    "```\n",
    "accuracy is 0.88757\n",
    "precision is 0.89267\n",
    "recall is 0.88757\n",
    "roc_auc is 0.97878\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:28.105893Z",
     "start_time": "2025-11-11T17:47:27.545691Z"
    }
   },
   "source": [
    "svm = SVC(\n",
    "    C=10,\n",
    "    kernel='rbf',\n",
    "    gamma='auto',\n",
    "    probability=True,\n",
    "    random_state=21,\n",
    "    class_weight=None\n",
    ")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "y_proba = svm.predict_proba(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='weighted')\n",
    "rec = recall_score(y_test, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test, y_proba, average='weighted', multi_class='ovo')\n",
    "\n",
    "print(f\"accuracy is {acc:.5f}\")\n",
    "print(f\"precision is {prec:.5f}\")\n",
    "print(f\"recall is {rec:.5f}\")\n",
    "print(f\"roc_auc is {roc_auc:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88757\n",
      "precision is 0.89267\n",
      "recall is 0.88757\n",
      "roc_auc is 0.97878\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The same task for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:28.200139Z",
     "start_time": "2025-11-11T17:47:28.170044Z"
    }
   },
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    class_weight='balanced',\n",
    "    criterion='gini',\n",
    "    max_depth=21,\n",
    "    random_state=21\n",
    ")\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "y_proba = tree.predict_proba(X_test)\n",
    "acc_tree = accuracy_score(y_test, y_pred)\n",
    "prec_tree = precision_score(y_test, y_pred, average='weighted')\n",
    "rec_tree = recall_score(y_test, y_pred, average='weighted')\n",
    "roc_auc_tree = roc_auc_score(y_test, y_proba, average='weighted', multi_class='ovo')\n",
    "\n",
    "print(f\"accuracy is {acc_tree:.5f}\")\n",
    "print(f\"precision is {prec_tree:.5f}\")\n",
    "print(f\"recall is {rec_tree:.5f}\")\n",
    "print(f\"roc_auc is {roc_auc_tree:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88462\n",
      "precision is 0.88765\n",
      "recall is 0.88462\n",
      "roc_auc is 0.93528\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The same task for random forest."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:28.480468Z",
     "start_time": "2025-11-11T17:47:28.263357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    criterion='entropy',\n",
    "    max_depth=24,\n",
    "    n_estimators=100,\n",
    "    random_state=21,\n",
    ")\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_proba = rfc.predict_proba(X_test) \n",
    "\n",
    "acc_rfc = accuracy_score(y_test, y_pred)\n",
    "prec_rfc = precision_score(y_test, y_pred, average='weighted')\n",
    "rec_rfc = recall_score(y_test, y_pred, average='weighted')\n",
    "roc_auc_rfc = roc_auc_score(y_test, y_proba, average='weighted', multi_class='ovo')\n",
    "\n",
    "print(f\"accuracy is {acc_rfc:.5f}\")\n",
    "print(f\"precision is {prec_rfc:.5f}\")\n",
    "print(f\"recall is {rec_rfc:.5f}\")\n",
    "print(f\"roc_auc is {roc_auc_rfc:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.92604\n",
      "precision is 0.92754\n",
      "recall is 0.92604\n",
      "roc_auc is 0.98939\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose the best model.\n",
    "2. Analyze: for which `weekday` your model makes the most errors (in % of the total number of samples of that class in your full dataset), for which `labname` and for which `users`.\n",
    "3. Save the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:28.557977Z",
     "start_time": "2025-11-11T17:47:28.543979Z"
    }
   },
   "source": [
    "metrics = [\n",
    "    {'name': 'svm', 'accuracy': acc, 'precision': prec, 'recall': rec, 'roc_auc': roc_auc, 'model': svm},\n",
    "    {'name': 'tree', 'accuracy': acc_tree, 'precision': prec_tree, 'recall': rec_tree, 'roc_auc': roc_auc_tree, 'model': tree},\n",
    "    {'name': 'rfc', 'accuracy': acc_rfc, 'precision': prec_rfc, 'recall': rec_rfc, 'roc_auc': roc_auc_rfc, 'model': rfc}\n",
    "]\n",
    "best_model = sorted(metrics, key=lambda x: (x['accuracy'], x['precision']), reverse=True)[0]\n",
    "print(f\"Best model: {best_model['name']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: rfc\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:28.819135Z",
     "start_time": "2025-11-11T17:47:28.622978Z"
    }
   },
   "source": [
    "best_model['model'].fit(X_train, y_train)\n",
    "y_pred_best = best_model['model'].predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=np.unique(y_test))\n",
    "errors = (cm.sum(axis=1) - np.diag(cm)) / cm.sum(axis=1)\n",
    "for i, day in enumerate(np.unique(y_test)):\n",
    "    print(f'Model makes the most errors {day}: {errors[i]:.2%}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model makes the most errors 0: 22.22%\n",
      "Model makes the most errors 1: 7.27%\n",
      "Model makes the most errors 2: 6.67%\n",
      "Model makes the most errors 3: 3.75%\n",
      "Model makes the most errors 4: 14.29%\n",
      "Model makes the most errors 5: 9.26%\n",
      "Model makes the most errors 6: 2.82%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:28.898583Z",
     "start_time": "2025-11-11T17:47:28.883746Z"
    }
   },
   "source": [
    "lab_cols = [col for col in X_test.columns if col.startswith('labname_')]\n",
    "for lab in lab_cols:\n",
    "    idx = X_test[lab] == 1\n",
    "    if idx.sum() == 0:\n",
    "        continue\n",
    "    err = (y_test[idx] != y_pred_best[idx]).sum() / idx.sum()\n",
    "    print(f'Model makes the most errors {lab}: {err:.2%}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model makes the most errors labname_code_rvw: 7.69%\n",
      "Model makes the most errors labname_lab03: 100.00%\n",
      "Model makes the most errors labname_lab03s: 0.00%\n",
      "Model makes the most errors labname_lab05s: 16.67%\n",
      "Model makes the most errors labname_laba04: 17.14%\n",
      "Model makes the most errors labname_laba04s: 8.00%\n",
      "Model makes the most errors labname_laba05: 2.13%\n",
      "Model makes the most errors labname_laba06: 11.11%\n",
      "Model makes the most errors labname_laba06s: 13.33%\n",
      "Model makes the most errors labname_project1: 5.38%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:28.990959Z",
     "start_time": "2025-11-11T17:47:28.960960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_cols = [col for col in X_test.columns if col.startswith('uid_user_')]\n",
    "for user in user_cols:\n",
    "    idx = X_test[user] == 1\n",
    "    if idx.sum() == 0:\n",
    "        continue\n",
    "    err = (y_test[idx] != y_pred_best[idx]).sum() / idx.sum()\n",
    "    print(f'Model makes the most errors {user}: {err:.2%}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model makes the most errors uid_user_1: 0.00%\n",
      "Model makes the most errors uid_user_10: 8.33%\n",
      "Model makes the most errors uid_user_12: 0.00%\n",
      "Model makes the most errors uid_user_13: 5.88%\n",
      "Model makes the most errors uid_user_14: 3.23%\n",
      "Model makes the most errors uid_user_15: 0.00%\n",
      "Model makes the most errors uid_user_16: 20.00%\n",
      "Model makes the most errors uid_user_17: 0.00%\n",
      "Model makes the most errors uid_user_18: 16.67%\n",
      "Model makes the most errors uid_user_19: 10.53%\n",
      "Model makes the most errors uid_user_2: 10.71%\n",
      "Model makes the most errors uid_user_20: 0.00%\n",
      "Model makes the most errors uid_user_21: 0.00%\n",
      "Model makes the most errors uid_user_22: 100.00%\n",
      "Model makes the most errors uid_user_23: 0.00%\n",
      "Model makes the most errors uid_user_24: 9.09%\n",
      "Model makes the most errors uid_user_25: 9.09%\n",
      "Model makes the most errors uid_user_26: 0.00%\n",
      "Model makes the most errors uid_user_27: 16.67%\n",
      "Model makes the most errors uid_user_28: 0.00%\n",
      "Model makes the most errors uid_user_29: 9.09%\n",
      "Model makes the most errors uid_user_3: 14.29%\n",
      "Model makes the most errors uid_user_30: 12.50%\n",
      "Model makes the most errors uid_user_31: 11.11%\n",
      "Model makes the most errors uid_user_4: 7.41%\n",
      "Model makes the most errors uid_user_6: 50.00%\n",
      "Model makes the most errors uid_user_8: 0.00%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:29.101985Z",
     "start_time": "2025-11-11T17:47:29.055959Z"
    }
   },
   "cell_type": "code",
   "source": "joblib.dump(best_model['model'], '../data/best_model.joblib')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/best_model.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that takes a list of different models and a corresponding list of parameters (dicts) and returns a dict that contains all the 4 metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:29.178984Z",
     "start_time": "2025-11-11T17:47:29.165988Z"
    }
   },
   "source": [
    "def written(models_with_params, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model, params in models_with_params:\n",
    "        if params:\n",
    "            model.set_params(**params)\n",
    "        y_pred = model.predict(X_test)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "        else:\n",
    "            y_proba = None\n",
    "\n",
    "        acc_written = accuracy_score(y_test, y_pred)\n",
    "        prec_written = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec_written = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        roc_auc_written = None\n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                roc_auc_written = roc_auc_score(y_test, y_proba, average='weighted', multi_class='ovo')\n",
    "            except ValueError:\n",
    "                roc_auc_written = None\n",
    "\n",
    "        results[name] = {\n",
    "            'accuracy': acc_written,\n",
    "            'precision': prec_written,\n",
    "            'recall': rec_written,\n",
    "            'roc_auc': roc_auc_written,\n",
    "            'params': params \n",
    "        }\n",
    "    return results\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:47:29.337289Z",
     "start_time": "2025-11-11T17:47:29.244985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_with_params = [\n",
    "    ('svm', svm, {'kernel': 'rbf', 'C': 10, 'gamma': 'auto'}),\n",
    "    ('tree', tree, {'max_depth': 21, 'criterion': 'gini'}),\n",
    "    ('rf', rfc, {'n_estimators': 100, 'max_depth': 24, 'criterion': 'entropy'})\n",
    "]\n",
    "\n",
    "metrics_dict = written(models_with_params, X_test, y_test)\n",
    "for name, metrics in metrics_dict.items():\n",
    "    print(f\"{name}: {metrics}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: {'accuracy': 0.8875739644970414, 'precision': 0.8926729169690374, 'recall': 0.8875739644970414, 'roc_auc': 0.9787793228216216, 'params': {'kernel': 'rbf', 'C': 10, 'gamma': 'auto'}}\n",
      "tree: {'accuracy': 0.8846153846153846, 'precision': 0.8876518218623483, 'recall': 0.8846153846153846, 'roc_auc': 0.935280206669359, 'params': {'max_depth': 21, 'criterion': 'gini'}}\n",
      "rf: {'accuracy': 0.9260355029585798, 'precision': 0.9275374670957044, 'recall': 0.9260355029585798, 'roc_auc': 0.9893851880258296, 'params': {'n_estimators': 100, 'max_depth': 24, 'criterion': 'entropy'}}\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
